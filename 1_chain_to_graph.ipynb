{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2faaaf4",
   "metadata": {},
   "source": [
    "# Need for Graph\n",
    "The key characteristic in LCEL chain is that the data flows in one direction. The prompt's output is piped to the model, and the model's output is piped to the parser. This structure is a Directed Acyclic Graph (DAG). It's \"directed\" because the data flows in a set direction, and \"acyclic\" because it contains no loops or cycles. Think of it as a factory assembly line: an item moves from one station to the next, but it never goes backward.\n",
    "\n",
    "**No Self correction, No Dynamic Re-Planning, Inability to Cycle**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1fe801",
   "metadata": {},
   "source": [
    "## The Solution: State and Cycles üîÑ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84d1047",
   "metadata": {},
   "source": [
    "**State**: Instead of passing a single output from one step to the next, all steps (called nodes) in a LangGraph share access to a central state object. Each node can read from this state and return updates to it. This state could contain the original input, chat history, intermediate steps, and tool outputs.\n",
    "\n",
    "**Cycles**: LangGraph allows you to define edges that connect nodes. Crucially, these edges can be conditional. This means you can create rules that say, \"After running the tool node, check the state. If there was an error, loop back to the LLM node. If it was successful, proceed to the final answer node.\" This allows us to explicitly create the loops that agentic behavior requires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4112e3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import TypedDict, Annotated\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, AnyMessage, ToolMessage\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bf3a04",
   "metadata": {},
   "source": [
    "The **state** is the heart of a LangGraph application. It's a central object that every node in the graph can read from and write to. We define its structure using a Python TypedDict\n",
    "\n",
    "simple chatbot to learn basics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4d27c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    \"\"\"represent the state of our graph\n",
    "    attributes:\n",
    "    messages: list of messages in the chat\"\"\"\n",
    "    messages:Annotated[list,add_messages] #annoted is a feature that lets you add metadata to a variable, \n",
    "    # like a comment or description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af933e46",
   "metadata": {},
   "source": [
    "Annotated[list, add_messages] means:\n",
    "‚ÄúThis field is a list, and it should be treated specially using the add_messages logic.‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916c73bd",
   "metadata": {},
   "source": [
    "TypedDict: This gives us a structured, predictable state object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1761431",
   "metadata": {},
   "source": [
    "Nodes are the \"workers\" of the graph. They are simple Python functions that perform a task. Each node function receives the current state as its only argument and must return a dictionary with the values to update in the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f13a93c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= GoogleGenerativeAI(model=\"gemini-2.0-flash\", api_key=google_api_key)\n",
    "\n",
    "def call_model(state:GraphState):\n",
    "    \"\"\"invokes model with current state\"\"\"\n",
    "    messages=state[\"messages\"]\n",
    "    response=model.invoke(messages)\n",
    "    return {\"messages\":[response]}\n",
    "# call_model() is a node that takes the current state of the graph and invokes the model with it. \n",
    "# It returns a new state with the model's response added to the messages list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b45990",
   "metadata": {},
   "source": [
    "### We'll create a StateGraph instance and define the nodes and the edges (the connections) between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c3e3ae",
   "metadata": {},
   "source": [
    "In LangGraph, a checkpointer is used to save and restore the state of your graph as it runs\n",
    "<ul>\n",
    "<li>Resuming: If your process stops or crashes, you can resume from the last saved state.</li>\n",
    "<li>Debugging: You can inspect previous states to see how your data changed.</li>\n",
    "<li>Persistence: You can keep a record of all the steps and messages in your workflow.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e313f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder= StateGraph(GraphState)\n",
    "\n",
    "graph_builder.add_node(\"llm\", call_model,description=\"Call the LLM with the current state\")\n",
    "\n",
    "graph_builder.set_entry_point(\"llm\")\n",
    "#simple edge that loops back to the LLM node\n",
    "graph_builder.set_finish_point(\"llm\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph=graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec373fd",
   "metadata": {},
   "source": [
    "Since this is a conversational chatbot we will run the graph in loop\n",
    "\n",
    "We create a config dictionary. The thread_id is crucial for the checkpointer to know which conversation's state to load and save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8163c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! How can I help you today?\n",
      "Hi Alex! It's nice to meet you. How can I help you today?\n",
      "Okay, so you want to know who \"the boss\" is. To help me answer that, I need a little more context. Are you asking:\n",
      "\n",
      "*   **In general?** Like, what is a boss?\n",
      "*   **About a specific company?** If so, which company?\n",
      "*   **About the TV show \"Who's the Boss?\"**\n",
      "\n",
      "Tell me more about what you're looking for!\n",
      "Okay, I understand you might be a little confused. Let's try this:\n",
      "\n",
      "I'm here to help you find information or answer questions. You asked \"whos the boss\".\n",
      "\n",
      "To give you the best answer, I need to know what you're asking about. Are you interested in:\n",
      "\n",
      "*   **Finding out who the boss is at a particular company?**\n",
      "*   **Learning about what a boss *is* in general?**\n",
      "*   **Something else entirely?**\n",
      "\n",
      "Just tell me what you're thinking!\n",
      "Exiting the chatbot.\n"
     ]
    }
   ],
   "source": [
    "config={\"configurable\":{\"thread_id\":\"first_convo\"}} #this is a configuration for the graph, it can be used to pass parameters to the graph\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Exiting the chatbot.\")\n",
    "        break\n",
    "    events=graph.stream(\n",
    "        {\"messages\":[HumanMessage(content=user_input)]},\n",
    "        config=config\n",
    "    )\n",
    "    # The 'values' attribute of the event contains the output of the node\n",
    "    \n",
    "    #event->{'llm': {'messages': ['Hello! How can I assist you?']}}\n",
    "    for event in events:\n",
    "        response = event['llm']['messages'][-1]\n",
    "        print(response)\n",
    "        # This prints the last message in the messages list, which is the model's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38de48a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
