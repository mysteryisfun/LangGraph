{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "663600cc",
   "metadata": {},
   "source": [
    "LangSmith is a platform built specifically for debugging, testing, and monitoring LLM applications. For LangGraph, it's an indispensable tool that gives you an X-ray view into your graph's execution, turning a complex black box into a transparent, explorable flowchart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705deb2c",
   "metadata": {},
   "source": [
    "**What LangSmith Provides**<br>\n",
    "Instead of trying to follow a messy stream of print statements, LangSmith gives you a rich, interactive UI where you can see:\n",
    "\n",
    "A full trace of your graph's execution path, node by node.\n",
    "\n",
    "The exact inputs (the state) each node received.\n",
    "\n",
    "The exact outputs (the state updates) each node produced.\n",
    "\n",
    "The full prompt and response for every LLM call.\n",
    "\n",
    "The inputs and outputs for every tool call.\n",
    "\n",
    "A \"diff\" view showing precisely how the state changed at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f96ca445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Hierarchical Agent Teams\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba468fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ujwal\\AppData\\Local\\Temp\\ipykernel_19996\\3929291860.py:12: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tool = TavilySearchResults(max_results=3)\n",
      "Unexpected argument 'gemini_api_key' provided to ChatGoogleGenerativeAI. Did you mean: 'google_api_key'?\n",
      "c:\\Users\\ujwal\\OneDrive\\Documents\\GitHub\\LangGraph\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3699: UserWarning: WARNING! gemini_api_key is not default parameter.\n",
      "                gemini_api_key was transferred to model_kwargs.\n",
      "                Please confirm that gemini_api_key is what you intended.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---EXECUTING HIERARCHICAL TEAM---\n",
      "---MANAGER: 2 topics left---\n",
      "---MANAGER: DELEGATING 'The future of AI hardware' TO WORKER---\n",
      "---WORKER: RETRIEVAL---\n",
      "---WORKER: ASSESSMENT---\n",
      "---WORKER ROUTER---\n",
      "---MANAGER: 1 topics left---\n",
      "---MANAGER: DELEGATING 'Recent advancements in large language models' TO WORKER---\n",
      "---WORKER: RETRIEVAL---\n",
      "---WORKER: ASSESSMENT---\n",
      "---WORKER ROUTER---\n",
      "\n",
      "\n",
      "================================================================================\n",
      "--- HIERARCHY EXECUTION COMPLETE ---\n",
      "Collected results:\n",
      "\n",
      "--- RESULT 1 ---\n",
      "Topic: The future of AI hardware\n",
      "\n",
      "```json\n",
      "{\n",
      "  'is_relevant': True\n",
      "}\n",
      "```\n",
      "\n",
      "--- RESULT 2 ---\n",
      "Topic: Recent advancements in large language models\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"is_relevant\": true\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "\n",
    "\n",
    "tool = TavilySearchResults(max_results=3)\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0, gemini_api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: CREATE THE WORKER GRAPH (Our Adaptive RAG Agent)\n",
    "# This is a self-contained agent that can research a topic and self-correct.\n",
    "# ==============================================================================\n",
    "\n",
    "class AdaptiveRAGState(TypedDict):\n",
    "    \"\"\"The state for our worker agent.\"\"\"\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "\n",
    "def create_adaptive_rag_graph():\n",
    "    \"\"\"Factory function to create the worker graph.\"\"\"\n",
    "    \n",
    "    # Define worker nodes\n",
    "    def retrieval_node(state: AdaptiveRAGState):\n",
    "        print(\"---WORKER: RETRIEVAL---\")\n",
    "        query = state['messages'][-1].content\n",
    "        retrieved_docs = tool.invoke({\"query\": query})\n",
    "        doc_text = \"\\n\\n\".join(str(d) for d in retrieved_docs)\n",
    "        return {\"messages\": [HumanMessage(content=doc_text, name=\"Retriever\")]}\n",
    "\n",
    "    def assessment_node(state: AdaptiveRAGState):\n",
    "        print(\"---WORKER: ASSESSMENT---\")\n",
    "        system_prompt = \"\"\"You are a relevance assessor. Your task is to evaluate if the retrieved documents are sufficient to answer the user's question.\n",
    "        Respond with a JSON object with one key, 'is_relevant': a boolean.\"\"\"\n",
    "        retrieved_docs = state['messages'][-1].content\n",
    "        user_question = state['messages'][-2].content\n",
    "        prompt = f\"User Question: {user_question}\\n\\nRetrieved Documents:\\n{retrieved_docs}\"\n",
    "        response = model.invoke([SystemMessage(content=system_prompt), HumanMessage(content=prompt)])\n",
    "        return {\"messages\": [HumanMessage(content=response.content, name=\"Assessor\")]}\n",
    "\n",
    "    def generation_node(state: AdaptiveRAGState):\n",
    "        print(\"---WORKER: GENERATION---\")\n",
    "        prompt = f\"Based on the following documents, please provide a comprehensive answer to this question:\\n\\nQuestion: {state['messages'][0].content}\\n\\nDocuments:\\n{state['messages'][-2].content}\"\n",
    "        response = model.invoke(prompt)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    # Define worker router\n",
    "    def relevance_router(state: AdaptiveRAGState) -> str:\n",
    "        print(\"---WORKER ROUTER---\")\n",
    "        assessment_message = state['messages'][-1].content\n",
    "        try:\n",
    "            assessment_json = json.loads(assessment_message)\n",
    "            if assessment_json.get('is_relevant'):\n",
    "                return \"generate\"\n",
    "            else:\n",
    "                return \"retrieve\" # Loop back to retrieve with the same query for simplicity\n",
    "        except (json.JSONDecodeError, KeyError):\n",
    "            return \"end\"\n",
    "\n",
    "    # Build the worker graph\n",
    "    builder = StateGraph(AdaptiveRAGState)\n",
    "    builder.add_node(\"retriever\", retrieval_node)\n",
    "    builder.add_node(\"assessor\", assessment_node)\n",
    "    builder.add_node(\"generator\", generation_node)\n",
    "    \n",
    "    builder.set_entry_point(\"retriever\")\n",
    "    builder.add_edge(\"retriever\", \"assessor\")\n",
    "    builder.add_conditional_edges(\"assessor\", relevance_router, {\n",
    "        \"generate\": \"generator\",\n",
    "        \"retrieve\": \"retriever\",\n",
    "        \"end\": END,\n",
    "    })\n",
    "    builder.add_edge(\"generator\", END)\n",
    "    \n",
    "    return builder.compile()\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: CREATE THE MANAGER GRAPH\n",
    "# This graph manages a list of tasks and delegates each one to the worker.\n",
    "# ==============================================================================\n",
    "\n",
    "# First, create an instance of our worker graph.\n",
    "research_worker_graph = create_adaptive_rag_graph()\n",
    "\n",
    "# Define the Manager's state\n",
    "class ManagerState(TypedDict):\n",
    "    topics_to_research: List[str]\n",
    "    current_topic: str\n",
    "    results: Annotated[List[str], lambda x, y: x + y] # Reducer to append results\n",
    "\n",
    "# Define the Manager's nodes\n",
    "def select_topic_node(state: ManagerState):\n",
    "    \"\"\"Selects the next topic from the list to be researched.\"\"\"\n",
    "    print(f\"---MANAGER: {len(state['topics_to_research'])} topics left---\")\n",
    "    topic = state['topics_to_research'][0]\n",
    "    remaining_topics = state['topics_to_research'][1:]\n",
    "    return {\"current_topic\": topic, \"topics_to_research\": remaining_topics}\n",
    "\n",
    "def researcher_proxy_node(state: ManagerState):\n",
    "    \"\"\"This is the key node. It invokes the worker graph.\"\"\"\n",
    "    topic = state['current_topic']\n",
    "    print(f\"---MANAGER: DELEGATING '{topic}' TO WORKER---\")\n",
    "    \n",
    "    # Invoke the worker graph with the current topic\n",
    "    worker_response = research_worker_graph.invoke(\n",
    "        {\"messages\": [HumanMessage(content=topic)]}\n",
    "    )\n",
    "    \n",
    "    # The worker's final answer is in its last message.\n",
    "    final_answer = worker_response['messages'][-1].content\n",
    "    \n",
    "    # Append the result to the manager's list of results\n",
    "    return {\"results\": [f\"Topic: {topic}\\n\\n{final_answer}\"]}\n",
    "\n",
    "# Define the Manager's router\n",
    "def manager_router(state: ManagerState):\n",
    "    \"\"\"Routes to the end if there are no more topics to research.\"\"\"\n",
    "    if not state['topics_to_research']:\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "# Build the Manager graph\n",
    "manager_builder = StateGraph(ManagerState)\n",
    "manager_builder.add_node(\"select_topic\", select_topic_node)\n",
    "manager_builder.add_node(\"research_worker\", researcher_proxy_node)\n",
    "\n",
    "manager_builder.set_entry_point(\"select_topic\")\n",
    "manager_builder.add_edge(\"select_topic\", \"research_worker\")\n",
    "manager_builder.add_conditional_edges(\"research_worker\", manager_router, {\n",
    "    \"continue\": \"select_topic\",\n",
    "    \"end\": END\n",
    "})\n",
    "\n",
    "manager_graph = manager_builder.compile()\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: EXECUTE THE HIERARCHY\n",
    "# ==============================================================================\n",
    "\n",
    "# The list of topics for the manager to delegate.\n",
    "topics = [\"The future of AI hardware\", \"Recent advancements in large language models\"]\n",
    "initial_state = {\"topics_to_research\": topics, \"results\": []}\n",
    "\n",
    "# Stream the results from the manager graph.\n",
    "print(\"\\n---EXECUTING HIERARCHICAL TEAM---\")\n",
    "final_result = manager_graph.invoke(initial_state)\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"--- HIERARCHY EXECUTION COMPLETE ---\")\n",
    "print(\"Collected results:\")\n",
    "for i, result in enumerate(final_result['results']):\n",
    "    print(f\"\\n--- RESULT {i+1} ---\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea9776b",
   "metadata": {},
   "source": [
    "### Streaming and Asynchronous Operations\n",
    " you use yield to turn a regular function into a generator that can stream out multiple values over time instead of just returning one value at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3cc9c296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---STREAMING WRITER---\n",
      "\n",
      "---STREAMING COMPLETE---\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from operator import add\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# --- 1. State and Model ---\n",
    "class StreamState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    # This will hold the streamed, final poem\n",
    "    poem: Annotated[str, lambda old, new: new]  # REPLACE, not add\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.7)\n",
    "\n",
    "# --- 2. The Streaming Node ---\n",
    "# This is the most important change.\n",
    "async def writer_node(state: StreamState):\n",
    "    \"\"\"A node that streams the output of an LLM.\"\"\"\n",
    "    print(\"---STREAMING WRITER---\")\n",
    "    # Get the topic from the last message\n",
    "    prompt = f\"Write a short, four-line poem about {state['messages'][-1].content}.\"\n",
    "    \n",
    "    # Use model.stream() to get a stream of tokens\n",
    "    stream = model.astream(prompt)\n",
    "    \n",
    "    # Yield a dictionary for each chunk from the stream\n",
    "    async for chunk in stream:\n",
    "        # Each chunk is a piece of the poem. We yield an update to the 'poem' key.\n",
    "        yield {**state,\"poem\": chunk.content}\n",
    "\n",
    "# --- 3. Build The Graph ---\n",
    "# A very simple graph: just the writer, which then ends.\n",
    "builder = StateGraph(StreamState)\n",
    "builder.set_entry_point(\"writer\")\n",
    "builder.add_node(\"writer\", writer_node)\n",
    "builder.add_edge(\"writer\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# --- 4. Consume the Stream with .astream_log() ---\n",
    "async def run_agent():\n",
    "    \"\"\"Runs the agent and prints the streamed output in real-time.\"\"\"\n",
    "    # We use .astream_log() to get a detailed stream of events.\n",
    "    full_poem = \"\"  # This will track the full poem as it builds\n",
    "    async for op in graph.astream_log(\n",
    "        {\"messages\": [HumanMessage(content=\"the moon\")],\"poem\":\"\"},\n",
    "        \n",
    "        config={\"include_values\":True} # This includes the full state view in the log\n",
    "    ):\n",
    "        # The log contains many operations. We're interested in the ones\n",
    "        # that update our 'poem' state.\n",
    "        for op in op.ops:\n",
    "            # We're interested in operations that update our 'poem' state.\n",
    "            # The path for this is '/values/poem'.\n",
    "            if op[\"op\"] == \"replace\" and op[\"path\"] == \"/values/poem\":\n",
    "                # op['value'] contains the full poem after this update.\n",
    "                # We diff it with the previous version to get the new chunk.\n",
    "                new_chunk = op[\"value\"][len(full_poem):]\n",
    "                print(new_chunk, end=\"\", flush=True)\n",
    "                full_poem = op[\"value\"] # Update our tracker\n",
    "    print(\"\\n---STREAMING COMPLETE---\")\n",
    "\n",
    "# To run an async function in a notebook or script:\n",
    "# In a Jupyter Notebook, you can just `await run_agent()`.\n",
    "# In a standard .py file, you run it like this:\n",
    "\"\"\"if __name__ == \"__main__\":\n",
    "    asyncio.run(run_agent())\"\"\"\n",
    "await run_agent()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31f5e4e",
   "metadata": {},
   "source": [
    "IDK why all the code in world is not making it stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e1f4c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---STREAMING WRITER---\n",
      "\n",
      "---STREAMING COMPLETE---\n"
     ]
    }
   ],
   "source": [
    "await run_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ef165b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending first chunk\n",
      "Received: First\n",
      "Resuming and sending second chunk\n",
      "Received: Second\n",
      "Resuming and sending final chunk\n",
      "Received: Third\n"
     ]
    }
   ],
   "source": [
    "def generator_function():\n",
    "    print(\"Sending first chunk\")\n",
    "    yield \"First\" # Pauses here\n",
    "    \n",
    "    print(\"Resuming and sending second chunk\")\n",
    "    yield \"Second\" # Pauses here\n",
    "    \n",
    "    print(\"Resuming and sending final chunk\")\n",
    "    yield \"Third\" # Pauses and ends\n",
    "\n",
    "# How you use it:\n",
    "for chunk in generator_function():\n",
    "    print(f\"Received: {chunk}\")\n",
    "\n",
    "# Output:\n",
    "# Sending first chunk\n",
    "# Received: First\n",
    "# Resuming and sending second chunk\n",
    "# Received: Second\n",
    "# Resuming and sending final chunk\n",
    "# Received: Third"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
