{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "663600cc",
   "metadata": {},
   "source": [
    "LangSmith is a platform built specifically for debugging, testing, and monitoring LLM applications. For LangGraph, it's an indispensable tool that gives you an X-ray view into your graph's execution, turning a complex black box into a transparent, explorable flowchart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705deb2c",
   "metadata": {},
   "source": [
    "**What LangSmith Provides**<br>\n",
    "Instead of trying to follow a messy stream of print statements, LangSmith gives you a rich, interactive UI where you can see:\n",
    "\n",
    "A full trace of your graph's execution path, node by node.\n",
    "\n",
    "The exact inputs (the state) each node received.\n",
    "\n",
    "The exact outputs (the state updates) each node produced.\n",
    "\n",
    "The full prompt and response for every LLM call.\n",
    "\n",
    "The inputs and outputs for every tool call.\n",
    "\n",
    "A \"diff\" view showing precisely how the state changed at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f96ca445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Hierarchical Agent Teams\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba468fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ujwal\\AppData\\Local\\Temp\\ipykernel_19996\\3929291860.py:12: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tool = TavilySearchResults(max_results=3)\n",
      "Unexpected argument 'gemini_api_key' provided to ChatGoogleGenerativeAI. Did you mean: 'google_api_key'?\n",
      "c:\\Users\\ujwal\\OneDrive\\Documents\\GitHub\\LangGraph\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3699: UserWarning: WARNING! gemini_api_key is not default parameter.\n",
      "                gemini_api_key was transferred to model_kwargs.\n",
      "                Please confirm that gemini_api_key is what you intended.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---EXECUTING HIERARCHICAL TEAM---\n",
      "---MANAGER: 2 topics left---\n",
      "---MANAGER: DELEGATING 'The future of AI hardware' TO WORKER---\n",
      "---WORKER: RETRIEVAL---\n",
      "---WORKER: ASSESSMENT---\n",
      "---WORKER ROUTER---\n",
      "---MANAGER: 1 topics left---\n",
      "---MANAGER: DELEGATING 'Recent advancements in large language models' TO WORKER---\n",
      "---WORKER: RETRIEVAL---\n",
      "---WORKER: ASSESSMENT---\n",
      "---WORKER ROUTER---\n",
      "\n",
      "\n",
      "================================================================================\n",
      "--- HIERARCHY EXECUTION COMPLETE ---\n",
      "Collected results:\n",
      "\n",
      "--- RESULT 1 ---\n",
      "Topic: The future of AI hardware\n",
      "\n",
      "```json\n",
      "{\n",
      "  'is_relevant': True\n",
      "}\n",
      "```\n",
      "\n",
      "--- RESULT 2 ---\n",
      "Topic: Recent advancements in large language models\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"is_relevant\": true\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "\n",
    "\n",
    "tool = TavilySearchResults(max_results=3)\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0, gemini_api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: CREATE THE WORKER GRAPH (Our Adaptive RAG Agent)\n",
    "# This is a self-contained agent that can research a topic and self-correct.\n",
    "# ==============================================================================\n",
    "\n",
    "class AdaptiveRAGState(TypedDict):\n",
    "    \"\"\"The state for our worker agent.\"\"\"\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "\n",
    "def create_adaptive_rag_graph():\n",
    "    \"\"\"Factory function to create the worker graph.\"\"\"\n",
    "    \n",
    "    # Define worker nodes\n",
    "    def retrieval_node(state: AdaptiveRAGState):\n",
    "        print(\"---WORKER: RETRIEVAL---\")\n",
    "        query = state['messages'][-1].content\n",
    "        retrieved_docs = tool.invoke({\"query\": query})\n",
    "        doc_text = \"\\n\\n\".join(str(d) for d in retrieved_docs)\n",
    "        return {\"messages\": [HumanMessage(content=doc_text, name=\"Retriever\")]}\n",
    "\n",
    "    def assessment_node(state: AdaptiveRAGState):\n",
    "        print(\"---WORKER: ASSESSMENT---\")\n",
    "        system_prompt = \"\"\"You are a relevance assessor. Your task is to evaluate if the retrieved documents are sufficient to answer the user's question.\n",
    "        Respond with a JSON object with one key, 'is_relevant': a boolean.\"\"\"\n",
    "        retrieved_docs = state['messages'][-1].content\n",
    "        user_question = state['messages'][-2].content\n",
    "        prompt = f\"User Question: {user_question}\\n\\nRetrieved Documents:\\n{retrieved_docs}\"\n",
    "        response = model.invoke([SystemMessage(content=system_prompt), HumanMessage(content=prompt)])\n",
    "        return {\"messages\": [HumanMessage(content=response.content, name=\"Assessor\")]}\n",
    "\n",
    "    def generation_node(state: AdaptiveRAGState):\n",
    "        print(\"---WORKER: GENERATION---\")\n",
    "        prompt = f\"Based on the following documents, please provide a comprehensive answer to this question:\\n\\nQuestion: {state['messages'][0].content}\\n\\nDocuments:\\n{state['messages'][-2].content}\"\n",
    "        response = model.invoke(prompt)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    # Define worker router\n",
    "    def relevance_router(state: AdaptiveRAGState) -> str:\n",
    "        print(\"---WORKER ROUTER---\")\n",
    "        assessment_message = state['messages'][-1].content\n",
    "        try:\n",
    "            assessment_json = json.loads(assessment_message)\n",
    "            if assessment_json.get('is_relevant'):\n",
    "                return \"generate\"\n",
    "            else:\n",
    "                return \"retrieve\" # Loop back to retrieve with the same query for simplicity\n",
    "        except (json.JSONDecodeError, KeyError):\n",
    "            return \"end\"\n",
    "\n",
    "    # Build the worker graph\n",
    "    builder = StateGraph(AdaptiveRAGState)\n",
    "    builder.add_node(\"retriever\", retrieval_node)\n",
    "    builder.add_node(\"assessor\", assessment_node)\n",
    "    builder.add_node(\"generator\", generation_node)\n",
    "    \n",
    "    builder.set_entry_point(\"retriever\")\n",
    "    builder.add_edge(\"retriever\", \"assessor\")\n",
    "    builder.add_conditional_edges(\"assessor\", relevance_router, {\n",
    "        \"generate\": \"generator\",\n",
    "        \"retrieve\": \"retriever\",\n",
    "        \"end\": END,\n",
    "    })\n",
    "    builder.add_edge(\"generator\", END)\n",
    "    \n",
    "    return builder.compile()\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: CREATE THE MANAGER GRAPH\n",
    "# This graph manages a list of tasks and delegates each one to the worker.\n",
    "# ==============================================================================\n",
    "\n",
    "# First, create an instance of our worker graph.\n",
    "research_worker_graph = create_adaptive_rag_graph()\n",
    "\n",
    "# Define the Manager's state\n",
    "class ManagerState(TypedDict):\n",
    "    topics_to_research: List[str]\n",
    "    current_topic: str\n",
    "    results: Annotated[List[str], lambda x, y: x + y] # Reducer to append results\n",
    "\n",
    "# Define the Manager's nodes\n",
    "def select_topic_node(state: ManagerState):\n",
    "    \"\"\"Selects the next topic from the list to be researched.\"\"\"\n",
    "    print(f\"---MANAGER: {len(state['topics_to_research'])} topics left---\")\n",
    "    topic = state['topics_to_research'][0]\n",
    "    remaining_topics = state['topics_to_research'][1:]\n",
    "    return {\"current_topic\": topic, \"topics_to_research\": remaining_topics}\n",
    "\n",
    "def researcher_proxy_node(state: ManagerState):\n",
    "    \"\"\"This is the key node. It invokes the worker graph.\"\"\"\n",
    "    topic = state['current_topic']\n",
    "    print(f\"---MANAGER: DELEGATING '{topic}' TO WORKER---\")\n",
    "    \n",
    "    # Invoke the worker graph with the current topic\n",
    "    worker_response = research_worker_graph.invoke(\n",
    "        {\"messages\": [HumanMessage(content=topic)]}\n",
    "    )\n",
    "    \n",
    "    # The worker's final answer is in its last message.\n",
    "    final_answer = worker_response['messages'][-1].content\n",
    "    \n",
    "    # Append the result to the manager's list of results\n",
    "    return {\"results\": [f\"Topic: {topic}\\n\\n{final_answer}\"]}\n",
    "\n",
    "# Define the Manager's router\n",
    "def manager_router(state: ManagerState):\n",
    "    \"\"\"Routes to the end if there are no more topics to research.\"\"\"\n",
    "    if not state['topics_to_research']:\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "# Build the Manager graph\n",
    "manager_builder = StateGraph(ManagerState)\n",
    "manager_builder.add_node(\"select_topic\", select_topic_node)\n",
    "manager_builder.add_node(\"research_worker\", researcher_proxy_node)\n",
    "\n",
    "manager_builder.set_entry_point(\"select_topic\")\n",
    "manager_builder.add_edge(\"select_topic\", \"research_worker\")\n",
    "manager_builder.add_conditional_edges(\"research_worker\", manager_router, {\n",
    "    \"continue\": \"select_topic\",\n",
    "    \"end\": END\n",
    "})\n",
    "\n",
    "manager_graph = manager_builder.compile()\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: EXECUTE THE HIERARCHY\n",
    "# ==============================================================================\n",
    "\n",
    "# The list of topics for the manager to delegate.\n",
    "topics = [\"The future of AI hardware\", \"Recent advancements in large language models\"]\n",
    "initial_state = {\"topics_to_research\": topics, \"results\": []}\n",
    "\n",
    "# Stream the results from the manager graph.\n",
    "print(\"\\n---EXECUTING HIERARCHICAL TEAM---\")\n",
    "final_result = manager_graph.invoke(initial_state)\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"--- HIERARCHY EXECUTION COMPLETE ---\")\n",
    "print(\"Collected results:\")\n",
    "for i, result in enumerate(final_result['results']):\n",
    "    print(f\"\\n--- RESULT {i+1} ---\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea9776b",
   "metadata": {},
   "source": [
    "### Streaming and Asynchronous Operations\n",
    "### For Real-Time Token Streaming:\n",
    "\n",
    "**If you want to see individual tokens as they're generated (like ChatGPT), you need astream_log() because:**\n",
    "\n",
    "<ol><li>graph.astream() only shows you the final result after each node finishes</li>\n",
    "<li>graph.astream_log() shows you every intermediate yield from your generator functions</li></ul>\n",
    "\n",
    "\n",
    " you use yield to turn a regular function into a generator that can stream out multiple values over time instead of just returning one value at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a868f1fa",
   "metadata": {},
   "source": [
    "using astream_log() for in depth metadata data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc9c296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected argument 'gemini_api_key' provided to ChatGoogleGenerativeAI. Did you mean: 'google_api_key'?\n",
      "c:\\Users\\ujwal\\OneDrive\\Documents\\GitHub\\LangGraph\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3699: UserWarning: WARNING! gemini_api_key is not default parameter.\n",
      "                gemini_api_key was transferred to model_kwargs.\n",
      "                Please confirm that gemini_api_key is what you intended.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---STARTING STREAMING AGENT---\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '',\n",
      "  'value': {'final_output': None,\n",
      "            'id': 'd0f345f5-dc0e-4db8-b186-c2d58ca69410',\n",
      "            'logs': {},\n",
      "            'name': 'LangGraph',\n",
      "            'streamed_output': [],\n",
      "            'type': 'chain'}})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/writer',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': '5c3153f4-c487-4a46-afb4-543705d89eaf',\n",
      "            'metadata': {'include_values': True,\n",
      "                         'langgraph_checkpoint_ns': 'writer:5a5da2d7-588f-81d0-b49f-754390e7690b',\n",
      "                         'langgraph_node': 'writer',\n",
      "                         'langgraph_path': ('__pregel_pull', 'writer'),\n",
      "                         'langgraph_step': 1,\n",
      "                         'langgraph_triggers': ('branch:to:writer',)},\n",
      "            'name': 'writer',\n",
      "            'start_time': '2025-07-26T13:46:38.520+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': ['graph:step:1'],\n",
      "            'type': 'chain'}})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/writer:2',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': '1409a30d-5ca6-4d76-9f56-7034b5fb6ac8',\n",
      "            'metadata': {'checkpoint_ns': 'writer:5a5da2d7-588f-81d0-b49f-754390e7690b',\n",
      "                         'include_values': True,\n",
      "                         'langgraph_checkpoint_ns': 'writer:5a5da2d7-588f-81d0-b49f-754390e7690b',\n",
      "                         'langgraph_node': 'writer',\n",
      "                         'langgraph_path': ('__pregel_pull', 'writer'),\n",
      "                         'langgraph_step': 1,\n",
      "                         'langgraph_triggers': ('branch:to:writer',)},\n",
      "            'name': 'writer',\n",
      "            'start_time': '2025-07-26T13:46:38.524+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': ['seq:step:1'],\n",
      "            'type': 'chain'}})\n",
      "---STREAMING WRITER---\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatGoogleGenerativeAI',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': 'a35a42a7-867f-44fe-ad05-0811551e161c',\n",
      "            'metadata': {'checkpoint_ns': 'writer:5a5da2d7-588f-81d0-b49f-754390e7690b',\n",
      "                         'include_values': True,\n",
      "                         'langgraph_checkpoint_ns': 'writer:5a5da2d7-588f-81d0-b49f-754390e7690b',\n",
      "                         'langgraph_node': 'writer',\n",
      "                         'langgraph_path': ('__pregel_pull', 'writer'),\n",
      "                         'langgraph_step': 1,\n",
      "                         'langgraph_triggers': ('branch:to:writer',),\n",
      "                         'ls_model_name': 'gemini-2.0-flash',\n",
      "                         'ls_model_type': 'chat',\n",
      "                         'ls_provider': 'google_genai',\n",
      "                         'ls_temperature': 0.7},\n",
      "            'name': 'ChatGoogleGenerativeAI',\n",
      "            'start_time': '2025-07-26T13:46:38.529+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': [],\n",
      "            'type': 'llm'}})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatGoogleGenerativeAI/streamed_output_str/-',\n",
      "  'value': 'A'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatGoogleGenerativeAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='A', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run--a35a42a7-867f-44fe-ad05-0811551e161c', usage_metadata={'input_tokens': 13, 'output_tokens': 0, 'total_tokens': 13, 'input_token_details': {'cache_read': 0}})})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/writer:2/streamed_output/-',\n",
      "  'value': {'messages': [HumanMessage(content='the moon', additional_kwargs={}, response_metadata={})],\n",
      "            'poem': 'A'}})\n",
      "ARunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatGoogleGenerativeAI/streamed_output_str/-',\n",
      "  'value': ' silver coin in'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatGoogleGenerativeAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' silver coin in', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run--a35a42a7-867f-44fe-ad05-0811551e161c', usage_metadata={'input_tokens': 0, 'total_tokens': 0, 'output_tokens': 0, 'input_token_details': {'cache_read': 0}})})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/writer:2/streamed_output/-',\n",
      "  'value': {'messages': [HumanMessage(content='the moon', additional_kwargs={}, response_metadata={})],\n",
      "            'poem': ' silver coin in'}})\n",
      " silver coin inRunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatGoogleGenerativeAI/streamed_output_str/-',\n",
      "  'value': ' velvet skies,\\nIt hangs above, a watchful eye.\\nGuiding tides'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatGoogleGenerativeAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' velvet skies,\\nIt hangs above, a watchful eye.\\nGuiding tides', additional_kwargs={}, response_metadata={'safety_ratings': []}, id='run--a35a42a7-867f-44fe-ad05-0811551e161c', usage_metadata={'input_tokens': 0, 'total_tokens': 0, 'output_tokens': 0, 'input_token_details': {'cache_read': 0}})})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/writer:2/streamed_output/-',\n",
      "  'value': {'messages': [HumanMessage(content='the moon', additional_kwargs={}, response_metadata={})],\n",
      "            'poem': ' velvet skies,\\n'\n",
      "                    'It hangs above, a watchful eye.\\n'\n",
      "                    'Guiding tides'}})\n",
      " velvet skies,\n",
      "It hangs above, a watchful eye.\n",
      "Guiding tidesRunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatGoogleGenerativeAI/streamed_output_str/-',\n",
      "  'value': \" with gentle grace,\\nA silent smile on night's dark face.\\n\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatGoogleGenerativeAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\" with gentle grace,\\nA silent smile on night's dark face.\\n\", additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--a35a42a7-867f-44fe-ad05-0811551e161c', usage_metadata={'input_tokens': -1, 'total_tokens': 35, 'output_tokens': 36, 'input_token_details': {'cache_read': 0}})})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/writer:2/streamed_output/-',\n",
      "  'value': {'messages': [HumanMessage(content='the moon', additional_kwargs={}, response_metadata={})],\n",
      "            'poem': ' with gentle grace,\\n'\n",
      "                    \"A silent smile on night's dark face.\\n\"}})\n",
      " with gentle grace,\n",
      "A silent smile on night's dark face.\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatGoogleGenerativeAI/final_output',\n",
      "  'value': {'generations': [[{'generation_info': {'finish_reason': 'STOP',\n",
      "                                                  'model_name': 'gemini-2.0-flash',\n",
      "                                                  'safety_ratings': []},\n",
      "                              'message': AIMessageChunk(content=\"A silver coin in velvet skies,\\nIt hangs above, a watchful eye.\\nGuiding tides with gentle grace,\\nA silent smile on night's dark face.\\n\", additional_kwargs={}, response_metadata={'safety_ratings': [], 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash'}, id='run--a35a42a7-867f-44fe-ad05-0811551e161c', usage_metadata={'input_tokens': 12, 'output_tokens': 36, 'total_tokens': 48, 'input_token_details': {'cache_read': 0}}),\n",
      "                              'text': 'A silver coin in velvet skies,\\n'\n",
      "                                      'It hangs above, a watchful eye.\\n'\n",
      "                                      'Guiding tides with gentle grace,\\n'\n",
      "                                      \"A silent smile on night's dark face.\\n\",\n",
      "                              'type': 'ChatGenerationChunk'}]],\n",
      "            'llm_output': None,\n",
      "            'run': None,\n",
      "            'type': 'LLMResult'}},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatGoogleGenerativeAI/end_time',\n",
      "  'value': '2025-07-26T13:46:39.465+00:00'})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/writer:2/final_output',\n",
      "  'value': {'messages': [HumanMessage(content='the moon', additional_kwargs={}, response_metadata={})],\n",
      "            'poem': ' with gentle grace,\\n'\n",
      "                    \"A silent smile on night's dark face.\\n\"}},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/writer:2/end_time',\n",
      "  'value': '2025-07-26T13:46:39.467+00:00'})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/writer/streamed_output/-',\n",
      "  'value': {'messages': [HumanMessage(content='the moon', additional_kwargs={}, response_metadata={})],\n",
      "            'poem': ' with gentle grace,\\n'\n",
      "                    \"A silent smile on night's dark face.\\n\"}})\n",
      " with gentle grace,\n",
      "A silent smile on night's dark face.\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/writer/final_output',\n",
      "  'value': {'messages': [HumanMessage(content='the moon', additional_kwargs={}, response_metadata={})],\n",
      "            'poem': ' with gentle grace,\\n'\n",
      "                    \"A silent smile on night's dark face.\\n\"}},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/writer/end_time',\n",
      "  'value': '2025-07-26T13:46:39.468+00:00'})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/streamed_output/-',\n",
      "  'value': {'writer': {'messages': [HumanMessage(content='the moon', additional_kwargs={}, response_metadata={})],\n",
      "                       'poem': ' with gentle grace,\\n'\n",
      "                               \"A silent smile on night's dark face.\\n\"}}},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': {'writer': {'messages': [HumanMessage(content='the moon', additional_kwargs={}, response_metadata={})],\n",
      "                       'poem': ' with gentle grace,\\n'\n",
      "                               \"A silent smile on night's dark face.\\n\"}}})\n",
      "\n",
      "---STREAMING COMPLETE---\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# --- 1. State and Model ---\n",
    "class StreamState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], ...]\n",
    "    poem: Annotated[str, lambda old, new: new]  # REPLACE, not add\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\", \n",
    "    temperature=0.7,\n",
    "    gemini_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "\n",
    "# --- 2. The Streaming Node ---\n",
    "async def writer_node(state: StreamState):\n",
    "    \"\"\"A node that streams the output of an LLM.\"\"\"\n",
    "    print(\"---STREAMING WRITER---\")\n",
    "    prompt = f\"Write a short, four-line poem about {state['messages'][-1].content}.\"\n",
    "    stream = model.astream(prompt)\n",
    "    \n",
    "    async for chunk in stream:\n",
    "        yield {**state, \"poem\": chunk.content}\n",
    "\n",
    "# --- 3. Build The Graph ---\n",
    "builder = StateGraph(StreamState)\n",
    "builder.set_entry_point(\"writer\")\n",
    "builder.add_node(\"writer\", writer_node)\n",
    "builder.add_edge(\"writer\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "async def run_agent():\n",
    "    \"\"\"Runs the agent and prints the streamed output in real-time.\"\"\"\n",
    "    print(\"---STARTING STREAMING AGENT---\")\n",
    "    async for patch in graph.astream_log(\n",
    "        {\"messages\": [HumanMessage(content=\"the moon\")], \"poem\": \"\"},\n",
    "        config={\"include_values\": True}\n",
    "    ):\n",
    "        i=0\n",
    "        if i==0:\n",
    "            print(patch)\n",
    "            i+=1\n",
    "        for op in patch.ops:\n",
    "            # Look for our writer node's streamed output\n",
    "            if (op[\"op\"] == \"add\" and \n",
    "                \"/logs/writer\" in op[\"path\"] and \n",
    "                \"/streamed_output/-\" in op[\"path\"]):\n",
    "                # Extract the poem content from the streamed output\n",
    "                value = op.get(\"value\", {})\n",
    "                if isinstance(value, dict) and \"poem\" in value:\n",
    "                    poem_chunk = value[\"poem\"]\n",
    "                    print(poem_chunk, end=\"\", flush=True)\n",
    "            \n",
    "            # Also catch the final complete poem\n",
    "            elif (op[\"op\"] == \"replace\" and op[\"path\"] == \"/final_output\"):\n",
    "                final_state = op.get(\"value\", {})\n",
    "                if \"poem\" in final_state:\n",
    "                    print(f\"\\n\\n--- FINAL POEM ---\\n{final_state['poem']}\")\n",
    "    \n",
    "    print(\"\\n---STREAMING COMPLETE---\")\n",
    "\n",
    "await run_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5206a5e",
   "metadata": {},
   "source": [
    "for astream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8b31206c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected argument 'gemini_api_key' provided to ChatGoogleGenerativeAI. Did you mean: 'google_api_key'?\n",
      "c:\\Users\\ujwal\\OneDrive\\Documents\\GitHub\\LangGraph\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3699: UserWarning: WARNING! gemini_api_key is not default parameter.\n",
      "                gemini_api_key was transferred to model_kwargs.\n",
      "                Please confirm that gemini_api_key is what you intended.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---STARTING SIMPLE STREAMING---\n",
      "---STREAMING WRITER---\n",
      "{'writer': {'messages': [HumanMessage(content='the moon', additional_kwargs={}, response_metadata={})], 'poem': ' with gentle sighs,\\nWhile the world softly sleeps.\\n'}}\n",
      "[WRITER]:  with gentle sighs,\n",
      "While the world softly sleeps.\n",
      "\n",
      "\n",
      "---SIMPLE STREAMING COMPLETE---\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# --- 1. State and Model ---\n",
    "class StreamState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], ...]\n",
    "    poem: Annotated[str, lambda old, new: new]  # REPLACE, not add\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\", \n",
    "    temperature=0.7,\n",
    "    gemini_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "\n",
    "# --- 2. The Streaming Node ---\n",
    "async def writer_node(state: StreamState):\n",
    "    \"\"\"A node that streams the output of an LLM.\"\"\"\n",
    "    print(\"---STREAMING WRITER---\")\n",
    "    prompt = f\"Write a short, four-line poem about {state['messages'][-1].content}.\"\n",
    "    stream = model.astream(prompt)\n",
    "    \n",
    "    async for chunk in stream:\n",
    "        yield {**state, \"poem\": chunk.content}\n",
    "\n",
    "# --- 3. Build The Graph ---\n",
    "builder = StateGraph(StreamState)\n",
    "builder.set_entry_point(\"writer\")\n",
    "builder.add_node(\"writer\", writer_node)\n",
    "builder.add_edge(\"writer\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "async def run_agent_simple():\n",
    "    \"\"\"Simple streaming - just gets the final state of each step.\"\"\"\n",
    "    print(\"---STARTING SIMPLE STREAMING---\")\n",
    "    async for step in graph.astream(\n",
    "        {\"messages\": [HumanMessage(content=\"the moon\")], \"poem\": \"\"}\n",
    "    ):\n",
    "        # Each 'step' is a dict with node_name: final_state\n",
    "        i=0\n",
    "        if i==0:\n",
    "            print(step)\n",
    "            i+=1\n",
    "        for node_name, state in step.items():\n",
    "            if \"poem\" in state and state[\"poem\"]:\n",
    "                print(f\"[{node_name.upper()}]: {state['poem']}\")\n",
    "    \n",
    "    print(\"\\n---SIMPLE STREAMING COMPLETE---\")\n",
    "\n",
    "await run_agent_simple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ef165b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending first chunk\n",
      "Received: First\n",
      "Resuming and sending second chunk\n",
      "Received: Second\n",
      "Resuming and sending final chunk\n",
      "Received: Third\n"
     ]
    }
   ],
   "source": [
    "def generator_function():\n",
    "    print(\"Sending first chunk\")\n",
    "    yield \"First\" # Pauses here\n",
    "    \n",
    "    print(\"Resuming and sending second chunk\")\n",
    "    yield \"Second\" # Pauses here\n",
    "    \n",
    "    print(\"Resuming and sending final chunk\")\n",
    "    yield \"Third\" # Pauses and ends\n",
    "\n",
    "# How you use it:\n",
    "for chunk in generator_function():\n",
    "    print(f\"Received: {chunk}\")\n",
    "\n",
    "# Output:\n",
    "# Sending first chunk\n",
    "# Received: First\n",
    "# Resuming and sending second chunk\n",
    "# Received: Second\n",
    "# Resuming and sending final chunk\n",
    "# Received: Third"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ed61bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---STARTING AGENT---\n",
      "---STREAMING WRITER STARTED---\n",
      "[CHUNK 1]: 'Silver'\n",
      "Silver[CHUNK 2]: ' orb in velvet skies,\n",
      "A watchful eye, where darkness lies.\n",
      "Pull'\n",
      " orb in velvet skies,\n",
      "A watchful eye, where darkness lies.\n",
      "Pull[CHUNK 3]: 'ing tides with gentle grace,\n",
      "A silent smile on night's dark face.\n",
      "'\n",
      "ing tides with gentle grace,\n",
      "A silent smile on night's dark face.\n",
      "---STREAMING WRITER FINISHED--- (Total chunks: 3)\n",
      "ing tides with gentle grace,\n",
      "A silent smile on night's dark face.\n",
      "\n",
      "---STREAMING COMPLETE---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "class StreamState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], ...]\n",
    "    poem: Annotated[str, lambda old, new: new]\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.7,\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "\n",
    "async def writer_node(state: StreamState):\n",
    "    print(\"---STREAMING WRITER STARTED---\")\n",
    "    prompt = f\"Write a short, four-line poem about {state['messages'][-1].content}.\"\n",
    "    stream = model.astream(prompt)\n",
    "    chunk_count = 0\n",
    "    async for chunk in stream:\n",
    "        chunk_count += 1\n",
    "        print(f\"[CHUNK {chunk_count}]: '{chunk.content}'\")  # Debug print\n",
    "        yield {**state, \"poem\": chunk.content}\n",
    "    print(f\"---STREAMING WRITER FINISHED--- (Total chunks: {chunk_count})\")\n",
    "\n",
    "builder = StateGraph(StreamState)\n",
    "builder.set_entry_point(\"writer\")\n",
    "builder.add_node(\"writer\", writer_node)\n",
    "builder.add_edge(\"writer\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "async def run_agent():\n",
    "    print(\"---STARTING AGENT---\")\n",
    "    async for patch in graph.astream_log(\n",
    "        {\"messages\": [HumanMessage(content=\"the moon\")], \"poem\": \"\"},\n",
    "        config={\"include_values\": True}\n",
    "    ):\n",
    "        for op in patch.ops:\n",
    "            # Look for the actual streaming output path\n",
    "            if op[\"op\"] == \"add\" and \"/streamed_output/-\" in op[\"path\"]:\n",
    "                # Check if it's a poem update (from our writer node)\n",
    "                if \"poem\" in str(op.get(\"value\", {})):\n",
    "                    poem_content = op[\"value\"].get(\"poem\", \"\")\n",
    "                    if poem_content:\n",
    "                        print(poem_content, end=\"\", flush=True)\n",
    "            \n",
    "            # Alternative: Look for final output\n",
    "            elif op[\"op\"] == \"replace\" and op[\"path\"] == \"/final_output\":\n",
    "                final_state = op.get(\"value\", {})\n",
    "                if \"poem\" in final_state:\n",
    "                    print(f\"\\n\\nFINAL POEM:\\n{final_state['poem']}\")\n",
    "    \n",
    "    print(\"\\n---STREAMING COMPLETE---\")\n",
    "\n",
    "await run_agent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc8511f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
