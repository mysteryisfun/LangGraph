{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d591454",
   "metadata": {},
   "source": [
    "### Why a Well-Designed State Matters\n",
    "Think of the State object as the single source of truth for your agent's entire operation. At any point in the graph's execution, the state tells you everything you need to know:\n",
    "\n",
    "What was the original question?\n",
    "\n",
    "What is the history of the conversation?\n",
    "\n",
    "What tools has the agent called?\n",
    "\n",
    "What were the results of those tool calls?\n",
    "\n",
    "Has the agent tried to re-plan? How many times?\n",
    "\n",
    "By structuring this information cleanly in a TypedDict, we make our nodes simpler to write and the entire graph's flow much easier to follow and debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b951757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65afe29",
   "metadata": {},
   "source": [
    "2 ways to update the memory:<br>\n",
    "Overwrite<br>\n",
    "append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "521e0698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class BasicState(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "def generate_answer(state:BasicState):\n",
    "    print(f\"Answering: {state['question']}\")\n",
    "    return {'answer':\"answer is ME\"}\n",
    "# If the initial state is {\"question\": \"What is the meaning of life?\", \"answer\": \"\"}\n",
    "# After running the node, the state becomes:\n",
    "# {\"question\": \"What is the meaning of life?\", \"answer\": \"The answer is 42.\"}\n",
    "# The empty string was overwritten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150190bb",
   "metadata": {},
   "source": [
    "2. Modify/Append (Using Annotated)<br>\n",
    "Often, we don't want to replace data; we want to add to it. This is where ***Annotated*** comes in. It lets you attach a reducer function to a key, which tells LangGraph how to merge the new value with the old one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4c24f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    tool_calls:Annotated[int, operator.add]\n",
    "def call_tool(state:AgentState):\n",
    "    print(\"calling the tool\")\n",
    "    # LangGraph will see the Annotated reducer and do:\n",
    "    # new_state['tool_calls'] = operator.add(old_state['tool_calls'], 1)\n",
    "    return{\"tools_calls\":1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44de94e",
   "metadata": {},
   "source": [
    "### Complex Schema\n",
    "state for a research agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6687b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "import operator\n",
    "\n",
    "class ResearchAgentState(TypedDict):\n",
    "    task:str\n",
    "    messages:Annotated[list[AnyMessage], add_messages]\n",
    "    documents:List[str]\n",
    "    replan_count:Annotated[int, operator.add]\n",
    "#Your State TypedDict is the blueprint for your agent's memory. Design it thoughtfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b8cf69",
   "metadata": {},
   "source": [
    "## If the State is the agent's memory and workbench, Nodes are the individual tools and workers that operate on it <br>\n",
    "LangGraph's design is that a node is nothing more than a standard Python function (or any callable) with a specific, simple signature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc66eb9",
   "metadata": {},
   "source": [
    "**The Node Signature: Input and Output**<br>\n",
    "Every function you want to use as a node must follow a simple contract:\n",
    "<ol>\n",
    "<li>Input: It must accept a single argument, which will be the current state object (an instance of your TypedDict).</li>\n",
    "\n",
    "<li>Output: It must return a dictionary. The keys of this dictionary must be a subset of the keys defined in your State TypedDict. The values will be used to update the state according to the rules we defined in the last lesson (overwrite or append/modify).</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061bce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchAgentState(TypedDict):\n",
    "    task: str\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    documents: List[str]\n",
    "    replan_count: Annotated[int, operator.add]\n",
    "\n",
    "def my_node_function(state: ResearchAgentState):\n",
    "    current_task = state['task']\n",
    "    current_messages= state['messages']\n",
    "    \n",
    "    print(\"perfonming REsearch.....\")\n",
    "    new_documents = [\"doc1\", \"doc2\"]  # Simulate some document retrieval\n",
    "    return {\"documents\": new_documents} #overwrite the documents key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316a337f",
   "metadata": {},
   "source": [
    "Best Practice: Single-Responsibility Nodes\n",
    "While you could write a single, massive node that calls an LLM, then parses its output, then decides which tool to use, and then executes it, this is a bad practice. It makes your graph monolithic, hard to debug, and difficult to reuse.\n",
    "\n",
    "The power of the graph architecture is realized when you break down your logic into small, modular, single-responsibility nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4670374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.messages.tool import ToolMessage\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "model= GoogleGenerativeAI(model=\"gemini-2.0-flash\",google_api_key=google_api_key)\n",
    "\n",
    "def call_planner(state: ResearchAgentState):\n",
    "    \"\"\"Decide the next step based on the current state.\"\"\"\n",
    "    #uses messages history to call llm\n",
    "    print(\"Planning next steps...\")\n",
    "    response = model.invoke(state['messages'])\n",
    "    return {\"messages\": [response]}  \n",
    "\n",
    "def execute_search(state: ResearchAgentState):\n",
    "    \"\"\"Perform a search based on the current task.\"\"\"\n",
    "    print(\"Executing search...\")\n",
    "    ai_message=state['messages'][-1]\n",
    "    \n",
    "    search_query= \"LangGraph\"\n",
    "    \n",
    "    result = f\"this is a search result for {search_query}\"\n",
    "    \n",
    "    # The result is returned as a ToolMessage to be added to the history\n",
    "    tool_message = ToolMessage(content=result, tool_call_id=ai_message.tool_calls[0]['id']) # A real ID would be here\n",
    "    return {\"documents\":[result], \"messages\": [tool_message]}\n",
    "\n",
    "def should_replan(state: ResearchAgentState) -> dict:\n",
    "    print(\"---RE-PLANNING---\")\n",
    "    # This node is called when the agent decides its first plan wasn't good enough.\n",
    "    replan_message = SystemMessage(content=\"The previous plan was not sufficient. I will try again.\")\n",
    "    return {\n",
    "        \"messages\": [replan_message],\n",
    "        \"replan_count\": 1 # This will be added to the existing count by operator.add\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d23735",
   "metadata": {},
   "source": [
    "## Conditional Edges for Intelligent Routing\n",
    "We have our State (the memory) and our Nodes (the workers). Now we need the managerâ€”the logic that directs the flow of work from one node to another. This is the role of Edges.<br>\n",
    "\n",
    "<ol>\n",
    "<li>Standard Edges (add_edge): This is a simple, unconditional connection. It says, \"After node A finishes, always go to node B.\" This is useful for linear sequences within your graph.</li>\n",
    "<li>Conditional Edges (add_conditional_edges): This is the decision-making mechanism. It says, \"After node A finishes, run a special routing function. This function will inspect the state and decide whether to go to node B, node C, or even end the process.\"</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1885e681",
   "metadata": {},
   "source": [
    "The routing function has the same signature as a node (it takes the state as input), but instead of returning a dictionary to update the state, it returns a string. This string is the name of the next node to execute.\n",
    "\n",
    "lets implement the call_planner node:<br>\n",
    "<ul><li>if llm's response contains req to use tool-> go to execute_search</li>\n",
    "<li>if the LLM response is a final answer we should finish the graph</li></ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97476032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "def route_after_planner(state:ResearchAgentState) ->str:\n",
    "    \"\"\"\n",
    "    Inspects the last message in the state and decides where to go next.\n",
    "\n",
    "    Returns:\n",
    "        A string that is the name of the next node.\n",
    "    \"\"\"\n",
    "    print(\"Routing after planner...\")\n",
    "    last_message = state['messages'][-1]\n",
    "    if isinstance(last_message, AIMessage):\n",
    "        print(\"AI wants to call a tool\")\n",
    "        return \"execute_search\"\n",
    "    else:\n",
    "        print(\"AI has finished its work\")\n",
    "        return \"end\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a14a34",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42572348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "graph_builder = StateGraph(ResearchAgentState)\n",
    "\n",
    "graph_builder.add_node(\"planner\", call_planner)\n",
    "\n",
    "graph_builder.add_node(\"execute_search\", execute_search)\n",
    "\n",
    "graph_builder.set_entry_point(\"planner\")\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"planner\", #decision is made after planner runs\n",
    "    \n",
    "    route_after_planner, #this function decides the next node\n",
    "    {\n",
    "        \"execute_search\":\"execute_search\",\n",
    "        \"end\":END\n",
    "    }\n",
    ")\n",
    "\n",
    "# After we execute the search, we ALWAYS want to go back to the planner\n",
    "# to let it see the results of the tool call. This creates our agentic loop!\n",
    "graph_builder.add_edge(\"execute_search\", \"planner\")\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a203a8",
   "metadata": {},
   "source": [
    "Start at planner.\n",
    "\n",
    "After planner, the route_after_planner function runs.\n",
    "\n",
    "If a tool is needed, it sends us to execute_search.\n",
    "\n",
    "After execute_search, the standard edge (add_edge) sends us back to planner, creating a cycle. The planner now sees the tool's results in the message history and can decide what to do next.\n",
    "\n",
    "If no tool is needed, the router sends us to END, and the graph finishes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b2b9e4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
